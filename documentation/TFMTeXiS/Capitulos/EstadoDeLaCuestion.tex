% !TeX encoding = ISO-8859-1

\chapter{State of the Art}
\label{State of the Art}

In this chapter, we have briefly defined Augmentative and Alternative Systems of Communication (section 2.1) and pictograms (section 2.2). Also, in section 2.3, we review Pict2Text 1.0, which is the tool that serves as the basis for our work.
In section 2.4. we present the machine learning models we have analyzed to solve our problem and their characteristics.


\section{Augmentative and Alternative Systems of Communication (AASC)}
The Augmentative and Alternative Systems of Communication (AASC)\footnote{\href{https://eprints.ucm.es/30164/1/Editor\%20predictivo\%20de\%20mensajes\%20en\%20pictogramas.pdf}{EDITOR PREDICTIVO DE MENSAJES EN PICTOGRAMAS}}$^{,}$\footnote{\href{https://eprints.ucm.es/48796/1/PICTAR.pdf}{PICTAR}}$^{,}$\footnote{\href{https://eprints.ucm.es/29245/1/ProyectoM\%C3\%A1ster_EspecialidadSistemasInteligentes_INTERSAACs_EvaGil.pdf}{INTERSAACs}} are communication systems, alternative to the natural language, that don't use spoken or written words but can transmit information. They are used by people, who cannot use natural language, or it is not sufficient for them to express themselves. They are created to increase the communication capabilities of the people who use them.

The AASCs do not arise naturally, but they need previous knowledge. There are two types of AASCs - those who need additional equipment such as objects, pictures, pictograms, etcetera, and those that do not need any equipment, depending on the needs, as they are often personalized to match the needs of its user. 

They include different systems of symbols: graphic and gestural. The gestural symbols can vary widely from mimicry to hand signs. The graphic symbols can be used by both people with an intellectual disability or with a motor disability. Examples of graphic symbols are drawings and pictures, as well as pictograms, which will be better explained in the next chapter.

Those systems provide various benefits for their users. They prevent or decrease the isolation of people with disabilities, helping with the improvement of social and communication abilities. Also, AASCs are relatively easy to learn and apply, and adapted for modern technology. 

\section{Pictograms}
Pictograms are written signs representing objects from the real world, as shown in Figure \ref{fig:icecream} - a pictogram of an icecream, ideas, actions, etcetera. In general, they represent anything that a person would want to express, without the need for verbal or written language. Pictograms are used in the day to day life at hospitals, malls, airports, etcetera because they are not strictly related to the spoken language. Pictograms are widely used by people with special needs, to help with communication and social integration. 

As pictograms are not universal, various systems exist, such as Blissymbolics, CSUP, Minspeak, ARASAAC, and more.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=50mm,scale=0.5]%
{Imagenes/Pict2Text2.0/icecream}
\caption{Pictogram representing an icecream.}
\label{fig:icecream}
\end{center}
\end{figure}

\subsection{Pictogram systems}

\subsubsection{Blissymbolics}
Blissymbolics\footnote{\href{https://eprints.ucm.es/48796/1/PICTAR.pdf}{PICTAR}}, an example of which you can see in Figure \ref{fig:Basicsymbols}, created in 1949, is an ideographic language consisting of several hundred basic symbols, each representing a concept. Each symbol is represented by basic forms (circles, triangles) and universal signs (numbers, punctuation signs) and uses colour codification to mark the grammar category. They can be combined to generate new symbols that represent new concepts. Blissymbolics characters do not correspond to the sounds of any spoken language and have their use in the education of people with communication difficulties.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=1\textwidth]%
{Imagenes/Pict2Text2.0/Basicsymbols}
\caption{Example of Blissymbolics}
\label{fig:Basicsymbols}
\end{center}
\end{figure}

\subsubsection{CSUP}
The Communication System Using Pictograms (CSP)\footnote{\href{https://eprints.ucm.es/48796/1/PICTAR.pdf}{PICTAR}}, developed in 1981 by Mayer-Johnson, is one of the systems that use pictograms to support interactive non-verbal communication. This system uses pictograms not only for objects but with events as well. It is designed in a way that it can be used between a person with a disability and a non-disabled person, child and adult, people speaking different languages, and so on. 

\subsubsection{Minspeak}
Minspeak (Semantic Compaction Systems, s.f.)\footnote{\href{https://eprints.ucm.es/48796/1/PICTAR.pdf}{PICTAR}} is a pictographic system created by Bruce Baker in 1992. Unlike other systems, this system is based on multi-meaning icons whose meaning is determined by the speech therapist and the user. As you in Figure \ref{fig:Minspeak}, two or three icons are combined, determined by rule-driven patterns, to code vocabulary.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=1\textwidth]%
{Imagenes/Pict2Text2.0/Minspeak}
\caption{Construction of symbologic in Minspeak.}
\label{fig:Minspeak}
\end{center}
\end{figure}

\subsubsection{ARASAAC}
The ARASAAC\footnote{\href{https://arasaac.org/}{https://arasaac.org/}}$^{,}$\footnote{\href{https://eprints.ucm.es/56627/1/1138049087-188701_JOS\%C3\%89_MAR\%C3\%8DA_L\%C3\%93PEZ_PULIDO_Entrega_TFG_Junio_Traductor_de_pictogramas_3940146_457579759.pdf}{Traductor de Pictogramas a Texto}}$^{,}$\footnote{\href{https://eprints.ucm.es/30164/1/Editor\%20predictivo\%20de\%20mensajes\%20en\%20pictogramas.pdf}{EDITOR PREDICTIVO DE MENSAJES EN PICTOGRAMAS}} system is the most used pictogram system in Spain. The ARASAAC project was created in 2007, and it currently consists of more than  30.000 pictograms, including complex pictograms with already constructed phrases, in more than 20 languages. The pictograms are separated into five groups - coloured pictograms, black and white, photographs, and sign language videos and pictures. Unlike other pictogram systems, ARASAAC makes a difference between singular and plural and gender differentiation, an example in Figure \ref{fig:profesors} where you can see the difference between the pictograms representing male and female teacher.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.3\textwidth]%
{Imagenes/Pict2Text2.0/profesor}
\includegraphics[width=0.3\textwidth]%
{Imagenes/Pict2Text2.0/profesora}
\caption{Example for gender diferenciation for the word `teacher`.}
\label{fig:profesors}
\end{center}
\end{figure}
 

As you can see in Figure \ref{fig:samepictogram}, one word can be represented by various pictograms. I the current example we can observe the word `teacher` having various representations.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.2\textwidth]%
{Imagenes/Pict2Text2.0/profesor}
\includegraphics[width=0.2\textwidth]%
{Imagenes/Pict2Text2.0/profesor1}
\includegraphics[width=0.2\textwidth]%
{Imagenes/Pict2Text2.0/profesor2}
\caption{Example for different pictograms representing the same word(teacher).}
\label{fig:samepictogram}
\end{center}
\end{figure}

Verbs come with a different pictogram for every conjugation, and the tense is determined by pictograms representing yesterday, today, and tomorrow. 

ARASAAC is free to use, internationally recognized, and provides a wide variety of pictograms. For those reasons, we decided to use the pictograms we can obtain from their website.

\subsection{Communication based on pictograms}
Communication via pictograms happens with the help of a board or a communication book, example of which you can see in Figure \ref{fig:board} - an ACC communication book, where the person points to the pictograms one by one to form a sentence. The complexity of the phrases is limited, usually consisting only of subject, verb, and object. Often, only the most significant words are used, although ARASAAC has pictograms for determinatives and prepositions.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=50mm,scale=0.5]%
{Imagenes/Pict2Text2.0/board}
\caption{Example of ACC communication book.}
\label{fig:board}
\end{center}
\end{figure}

\section{Pict2Text 1.0}
As described previously, Pict2Text 1.0\footnote{\href{https://eprints.ucm.es/56627/1/1138049087-188701_JOS\%C3\%89_MAR\%C3\%8DA_L\%C3\%93PEZ_PULIDO_Entrega_TFG_Junio_Traductor_de_pictogramas_3940146_457579759.pdf}{Traductor de Pictogramas a Texto}} is the initial state and the base of our project. The first version of this project is a web application that permits the translation of sentences written using pictograms to natural language (Spanish). 


The following images and descriptions present a simple flow of actions a user can do to translate a message written with pictograms into natural language.
When entering the website\footnote{\href{https://holstein.fdi.ucm.es/tfg-pict2text}{https://holstein.fdi.ucm.es/tfg-pict2text}} the user can see on the left part, a big panel, the pictogram sentence panel, with a caption `Pictograms` above it, and a button `Traducir` below it. On the right part, an input box with the caption `Nombre del picto`, and a button `Buscar` on the left of it. 
We can write and search a specific word from the ARASAAC pictogram database and display it into a panel on the right part of the web page. After that, we can include the chosen one into the pictogram sentence panel, from where later the message is translated into natural language.

To search for a specific pictogram, the user should write the world they are looking for in the input box of the right side and click the button 'Buscar' as shown in Figure \ref{fig:pict2text_v1_2}, where we can see the search of the word 'Hombre' and the corresponding pictogram.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=1\textwidth]%
{Imagenes/Pict2Text2.0/pict2text_v1_2}
\caption{Searching the word "Hombre"}
\label{fig:pict2text_v1_2}
\end{center}
\end{figure}


After searching the pictogram, the user needs to include it into the left panel with pictograms. This is done by clicking the button ``A\~{n}adir''. As we can observe in Figure \ref{fig:pict2text_v1_3}, the pictogram corresponding to the word "Hombre" is included in the pictogram sentence panel.


\begin{figure}[!ht]
\begin{center}
\includegraphics[width=1\textwidth]%
{Imagenes/Pict2Text2.0/pict2text_v1_3}
\caption{Adding the pictogram "Hombre" to the pictogram sentence panel}
\label{fig:pict2text_v1_3}
\end{center}
\end{figure}

We can form a sentence by repeating the previous step with other words. Figure \ref{fig:pict2text_v1_4} displays a translation of a sentence written with the pictograms corresponding to the words ``Hombre'', ``Comer'', ``pizza''. As we can see that is translated to "El hombre come una pizza" ("The man is eating a pizza").

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=1\textwidth]%
{Imagenes/Pict2Text2.0/pict2text_v1_4}
\caption{Translating the sentence "El hombre come una pizza."}
\label{fig:pict2text_v1_4}
\end{center}
\end{figure}

\subsection{Implementation}
For the front-end of the project, Angular\footnote{\href{https://angular.io/}{Angular}} was used. As the website itself is a SPA (Single-Page Applications), which needs to respond fast, a framework like Angular fulfills this performance requirement. 

As all of the different functionalities from the project were implemented as web services, most of them in Python, the team of `Pict2Text` version 1 have decided to use the framework Django\footnote{\href{https://www.djangoproject.com/}{Django}} for integration and intercommunication between them.

The core of Pict2Text 1.0 is the API of ARASAAC\footnote{\href{https://arasaac.org/developers/api}{ARASAAC API}}. It provides the searching mechanism used to match words to pictograms, the graphical images of pictograms, and additional information about them. 

A generator of grammatically correct phrases in Spanish was needed. SimpleNLG\footnote{\href{https://github.com/simplenlg/simplenlg}{SimpleNLG}} was used in Pict2Text 1.0, a Java library for natural language generation. This library permits the creation of simple and complex phrases. To do that, it requires sentence structure- subject, verb, adjectives, gender, and number (singular or plural) of every word in the formed sentence. With this information, SimpleNLG can generate grammatically correct sentences.

Spacy\footnote{\href{https://spacy.io/}{Spacy}} is the tool that gives the previous word characteristics. It is a python library with a high accuracy used for advanced natural language processing.

\subsection{Conclusions}
Although the first version of Pict2Text translates the pictograms into natural language, it requires the user to manually select the pictograms they want to use in the construction of their sentence. Writing the words is impossible for people with disabilities who need pictograms to communicate if it was not, they would have used the natural language in the first place. 

This problem can be solved, giving those people the option to upload a picture of a sentence constructed with pictograms. The functionality we are building will be able to separate the different pictograms from the original image and later translate the phrase using the implementation in Pict2Text 1.0. 

Another issue that exists in this project is the translation of pictograms to natural language. The algorithm used allows the correct translation of simple phrases with one subject and one verb. One of our goals is to improve that performance and provide the opportunity to construct complex sentences with a high level of assertion. 

\section{Machine learning algorithms}
\subsection{Why a conventional algorithm is not going to resolve the problem?}
As the problem we have is to detect pictograms from a given image, we cannot assume that a specific pictogram has one, and only one, graphical representation. Although being relatively close for a particular topic, the images of the different pictograms are not the same. Their color and sometimes format varies.

In ARASAAC, searching for the word `man` returns several different pictograms. The first two results are shown in the Figure \ref{fig:men}, and they are an exact match to the searched word. The first is a colored pictogram, and the second a grayscale\footnote{\href{https://en.wikipedia.org/wiki/Grayscale}{Grayscale}} one.


\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.3\textwidth]%
{Imagenes/Pict2Text2.0/man_colored}
\includegraphics[width=0.3\textwidth]%
{Imagenes/Pict2Text2.0/man_grayscale}
\caption{Colored and grayscale pictograms representing the word "man"} 
\label{fig:men}
\end{center}
\end{figure}

But the different colors and formats are not the only problem why we cannot compare them pixel by pixel. Both of the pictograms above are of a man. As we can observe, in the first, the man is represented with a mouth, a nose, eyes, and lines separating his shirt from his trousers, and everything is in colour. The second one has the same form, but all filled in black. Despite the similarities, a comparison between the two pictograms pixel by pixel will indicate that the images are not the same. Other pictograms differ in shape, as well as in colour. Considering that, we determined that without a machine learning model, capable of comparing images, we will not be able to solve our problem.

\subsection{What is a Machine Learning and Machine Learning model?}
Machine learning (ML)\footnote{\href{https://en.wikipedia.org/wiki/Machine_learning}{Machine learning}} is the study of computer algorithms that improve automatically through experience. It is seen as a subset of artificial intelligence. Machine learning algorithms build a model based on sample data, known as "training data", to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.

A machine learning model\footnote{\href{https://docs.microsoft.com/en-us/windows/ai/windows-ml/what-is-a-machine-learning-model}{What is a machine learning model?}} is a file that has been trained to recognize certain types of patterns. You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from those data.
Once you have trained the model, you can use it to reason over data that it hasn't seen before, and make predictions about those data

\subsection{Why no other tool or ML model is helpful to resolve our problem?}
After analyzing the ARASAAC Spanish pictograms - our dataset, we realized there are several difficulties and problems we have to consider. The main one is that there are above 8000 pictograms and more than 6000 represent different concepts. 
Having a dataset with as few representatives of the different classes as ours does not apply to most of the ML models. To train a model from scratch a dataset with a lot of representatives of the different classes is needed. In our case with a single or less than 3 applying even the simplest shallow Neural Network, Logistic Regression will overfit to the data and gives a high but unrealistic accuracy. Providing images not included in the original dataset to an overfitted model will show poor accuracy and incorrect results. With a more complex Neural Network model, the situation described above  will be the same, even worse, because they need even more data as they find better functions and more distinct features of the given images.

\subsection{What is a Neural Network?}
Artificial neural networks (ANNs)\footnote{\href{https://en.wikipedia.org/wiki/Artificial_neural_network}{Artificial neural network}}, usually called neural networks (NNs), are computing systems vaguely inspired by the biological neural networks that constitute animal brains.
An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The "signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, as presented in Figure \ref{fig:NN}, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.
   
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=50mm,scale=0.5]%
{Imagenes/Pict2Text2.0/NN}
\caption{A simple neural network architecture.}
\label{fig:NN}
\end{center}
\end{figure}

Supervised learning is learning through pre-labelled inputs, which act as targets. For each training example there will be a set of input values (vectors) and one or more associated designated output values. The goal of this form of training is to reduce the models overall classification error, through correct calculation of the output value of training example by training. 
Unsupervised learning differs in that the training set does not include any labels. Success is usually determined by whether the network is able to reduce or increase an associated cost function. However, it is important to note that most image-focused pattern-recognition tasks usually depend on classification using supervised learning\footnote{\href{https://arxiv.org/pdf/1511.08458.pdf}{An Introduction to Convolutional Neural Networks}}.


\subsection{Convolutional Neural Network}

Convolutional Neural Networks (CNNs)\footnote{\href{https://arxiv.org/pdf/1511.08458.pdf}{An Introduction to Convolutional Neural Networks}} are analogous to traditional ANNs in that they are composed of neurons that self-optimize through learning. Each neuron will still receive input and perform an operation, such as a scalar product followed by a nonlinear function - the basis of countless ANNs. From the input raw image vectors to the final output of the class score, the entire network will still express a single perceptive score function (the weight). The last layer will contain loss functions associated with the classes.
The only notable difference between CNNs and traditional ANNs is that CNNs are primarily used in the field of pattern recognition within images. This allows us to encode image-specific features into the architecture, making the network more suited for image-focused tasks - whilst further reducing the parameters required to set up the model. 
One of the largest limitations of traditional forms of ANN is that they tend to struggle with the computational complexity required to compute image data. Common machine learning benchmarking datasets such as the MNIST\footnote{\href{http://yann.lecun.com/exdb/mnist/}{MNIST}} database of handwritten digits are suitable for most forms of ANN, due to its relatively small image dimensionality of just 28 x 28. With this dataset, a single neuron in the first hidden layer will contain 784 weights (28x28x1 where 1 bare in mind that MNIST is normalised to just black and white values), which is manageable for most forms of ANN. If you consider a more substantial coloured image input of 64 x 64, the number of weights on just a single neuron of the first layer increases substantially to 12288(64x64x3- Height, Width, RGB\footnote{\href{https://en.wikipedia.org/wiki/RGB_color_model}{RGB}}
 channels). Also, take into account that to deal with this scale of input, the network will also need to be a lot larger than one used to classify colour-normalised MNIST digits, then you will understand the drawbacks of using such models.
 
\subsubsection{Convolution and Convolution operation}
The convolution operation\footnote{\href{https://en.wikipedia.org/wiki/Convolution}{Convolution}} is one of the fundamental building blocks of a Convolutional Neural Network.
Although in mathematics (in particular, functional analysis), convolution is a mathematical operation on two functions (f and g) that produces a third function that expresses how the shape of one is modified by the other, in computer vision, this term has a slightly different meaning (the convolution as described in the use of convolutional neural networks, bellow, is a 'cross-correlation').

In the context of a convolutional neural network, convolution is a linear operation that involves the multiplication of a set of weights with the input, much like in a traditional neural network. Given that the technique was designed for two-dimensional input, the multiplication is performed between an array of input data and a two-dimensional array of weights, called a filter or a kernel.
The filter is smaller than the input data, and the type of multiplication is applied between a filter-sized patch of the input, and the filter is a dot product. A dot product is an element-wise multiplication between the filter-sized patch of the input and filter, which is then summed, always resulting in a single value. Because it results in a single value, the operation is often referred to as the `scalar product`.
Using a filter smaller than the input is intentional as it allows the same filter (set of weights) to be multiplied by the input array multiple times at different points on the input. Specifically, the filter is applied systematically to each overlapping part or filter-sized patch of the input data, left to right, top to bottom.
If the filter is designed to detect a specific type of feature in the input, then the application of that filter systematically across the entire input image allows the filter an opportunity to discover that feature anywhere in the image.
The output from multiplying the filter with the input array one time is a single value. As the filter is applied multiple times to the input array, the result is a two-dimensional array of output values that represent the filtering of the input. As such, the two-dimensional output array from this operation is called a `feature map`\footnote{\href{https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/}{How Do Convolutional Layers Work in Deep Learning Neural Networks?}}- Figure \ref{fig:convolution}.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=1\textwidth]%
{Imagenes/Pict2Text2.0/convolution}
\caption{Filtering over the input image and constructing the feature map.}
\label{fig:convolution}
\end{center}
\end{figure}

As you can see in the Figure \ref{fig:convolution_operation}, 3 by 3 filter for vertical edge detection applied to an image with height 6 and width 6 with stride 1 and no padding.


\begin{figure}[!ht]
\begin{center}
\includegraphics[width=1\textwidth]%
{Imagenes/Pict2Text2.0/convolution_operation}
\caption{Filtering over an input image, representing the convolution operation in CNN.}
\label{fig:convolution_operation}
\end{center}
\end{figure}

\subsubsection{Architecture}
A typical CNN starts with the input image, sent to a network, formed by convolutional and pooling layers where the learning of the features of the image is performed. Later the final result of these is flattened, forming the input of the fully connected layers (neural network layers) with a classification (activation) function in the end. This way, the characteristics of the input image are detected in the beginning, and later, the picture is classified according to them. IN Figure \ref{fig:cnn_architecture} you can see a CNN architecture.


\begin{figure}[!ht]
\begin{center}
\includegraphics[width=1\textwidth]%
{Imagenes/Pict2Text2.0/CNN-architecture}
\caption{A convolutional neural network formed by several features layers followed by classifications.}
\label{fig:CNN-architecture}
\end{center}
\end{figure}








