% !TeX encoding = ISO-8859-1
\chapter{Implementation}
\label{Implementation}

In this chapter, we are going to present all we have developed for Pict2Text 2.0. In section 4.1 we describe the pre required scripts we had created to download the dataset and preprocess it. As one of the main goals of this project is to provide the functionality to import a picture of a message written with pictograms and translate it, we had to construct a machine learning model to do that in 4.2 we describe the actual implementation of it. 

\section{Scripts for fetching and loading ARASAAC pictograms}
We started by obtaining our dataset, the ARASAAC pictograms in Spanish. We created a python script that uses a REST client to call the ARASAAC API. The script does a request to the ARASAAC API, returning the information of all pictograms in Spanish. Later it downloads every one of them calling to the same API. The fetched pictograms are stored locally in separate folders with a maximum size of 4000 images. As the process of downloading the pictograms required more than 3 hours for the complete fetching of over 11000 images, we included concurrency and reduced the execution time to under an hour. 

Having the dataset fetched we had to load it in memory in a way that the model we had constructed could use it. We have implemented another python script to achieve that. In it, we have used the image preprocessing module of Keras, to read the pictograms from a directory and store them in memory. All ARASAAC pictograms are in the image format png and during loading, we had to configure the module of Keras to load them as four-color channels- RGBA and resize them to 105 height by 105 width.

In the pictogram dataset, for most of the words, there was a single pictogram representing an actual concept. Due to the lack of images, a machine learning model could not be able to extract a pattern for every concept, hence the algorithm would provide poor results. As a result of that, during the preprocessing of the dataset, we needed to augment the dataset and generate different representatives of every one of the pictograms. To achieve that, we had created three augmentations scripts. All of them use Keras's preprocessing module but they differ in the way they manipulate the pictograms. Keras's module includes the class ImageDataGenerator which comes with different image manipulation and augmentation capabilities like changing the brightness of an image, rotating it, zooming it, and more.

The first script changes the brightness of the pictogram. We provided a range between 0.2 and 1.0 to the ImageDataGenerator, specifying that the augmented image brightness should take a random value in that range. The two extremes represent a very dark and very bright image. In Figure \ref{fig:bee} is shown the original pictogram of a bee(`abeja`).

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=50mm,scale=0.5]%
{Imagenes/Pict2Text2.0/bee}
\caption{The original image of the pictogram bee(`abeja`) from the ARASAAC.}
\label{fig:bee}
\end{center}
\end{figure}

In figure \ref{fig:beeShade} are shown augmented images of the pictogram bee(`abeja`) using our random brightness augmentation script.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=25mm,scale=0.25]%
{Imagenes/Pict2Text2.0/beeShade}
\includegraphics[width=25mm,scale=0.25]%
{Imagenes/Pict2Text2.0/beeShade2}
\includegraphics[width=25mm,scale=0.25]%
{Imagenes/Pict2Text2.0/beeShade3}
\includegraphics[width=25mm,scale=0.25]%
{Imagenes/Pict2Text2.0/beeShade4}
\caption{Augmented images of the pictogram bee(`abeja`) using the random brightness augmentation script.}
\label{fig:beeShade}
\end{center}
\end{figure}

This type of augmentation will help the model to detect the concept represented in the pictogram independently of the light or brightness conditions from the provided image.

The second augmentation script rotates the pictogram randomly. Similar to the brightness augmentation script this one is using the ImageDataGenerator class, specifying the rotation\_range attribute to 90 degrees. This specification will augment the provided pictogram generating random 90 degrees rotations. In Figure \ref{fig:beeUpDown} are shown two augmented images of the bee pictogram using the random augmentation script.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=30mm,scale=0.5]%
{Imagenes/Pict2Text2.0/beeUp}
\includegraphics[width=30mm,scale=0.5]%
{Imagenes/Pict2Text2.0/beeDown}
\caption{Two 90 degree rotations of the pictogram bee(`abeja`) generated using the augmentation script.}
\label{fig:beeUpDown}
\end{center}
\end{figure}

This type of augmentation will help the model to recognize the pictogram even if the provided image of it is rotated or tilted aside.

The third augmentation script changes the color of the pictogram. The script iterates through the width and height of the given pictogram and changes randomly the RGB channels of it. The script does not change the alpha channel of the image as we would like to keep the original, in most cases transparent, background. In Figure \ref{fig:beeColour} are shown four augmentations of the pictogram bee(`abeja`) using the color augmentation script.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=25mm,scale=0.25]%
{Imagenes/Pict2Text2.0/beeColour}
\includegraphics[width=25mm,scale=0.25]%
{Imagenes/Pict2Text2.0/beeColour2}
\includegraphics[width=25mm,scale=0.25]%
{Imagenes/Pict2Text2.0/beeColour3}
\includegraphics[width=25mm,scale=0.25]%
{Imagenes/Pict2Text2.0/beeColour4}
\caption{Four color augmented images of the pictogram bee(`abeja`) generated using the color augmentation script.}
\label{fig:beeColour}
\end{center}
\end{figure}

This type of augmentation will help the model to find the pictogram even if the provided image of it has different colors.

\section{One-shot learning algorithm}

For the implementation of the one-shot learning algorithm, we have taken an already implemented one from the \emph{One-Shot Learning with Siamese Networks using Keras} \cite{oneShot}. The implementation shown in the article is suitable for our needs, given that the problem with letter recognition is similar to the one with pictogram recognition. 

A difference between the two problems - the one solved in the article and the one we are solving, is the dataset. The original algorithm is implemented to work with a dataset formed by pictures of letters from different alphabets. The aforementioned letters the original algorithm recognizes are in grayscale, and our pictograms are in PNG format. As shown in Figure \ref{fig:grayscale}, in machine learning a picture in grayscale is represented by a two-dimensional matrix, with dimensions the width and the height of the picture. Each cell corresponds to 1 pixel and contains a number between 0 and 255, representing the shade of grey where 0 is black and 255 is white. On the other hand, as you can see in Figure \ref{fig:rgb}, the colorful images need three matrices for the channels red, green, and blue for every pixel. In our case, we have a fourth matrix representing the alpha channel (the opacity of the picture), as it is included in the RGBA, png images we use. For that reason, we had to include one additional layer to the input matrices used by the algorithm for the colors and the opacity of our data.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=75mm,scale=0.5]%
{Imagenes/Pict2Text2.0/grayscale}
\caption{Image represented in grayscale image and the corresponding matrix.}
\label{fig:grayscale}
\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=75mm,scale=0.75]%
{Imagenes/Pict2Text2.0/reign_pic_breakdown}
\includegraphics[width=50mm,scale=0.25]%
{Imagenes/Pict2Text2.0/three_d_array}
\caption{Colourful RGB image with its RGB three matrices.}
\label{fig:rgb}
\end{center}
\end{figure}

First, we tried the algorithm with fewer data. We have used three data sets - training, validation, and test sets. The training set was used to train the model and achieve the weights we will need to recognize given pictograms, later when we ask the model to predict. The validation set was used to compare the accuracy of the algorithm on different data in order to tune the hyperparameters, like the number of iterations. The last one, the test set, we used to provide an unbiased evaluation of the model, simulating a near-real situation where the model would face pictograms never seen by it before. We use the test set to compute how similar a given pictogram is to all other pictograms forming the set.

For the training set, we used 22 pictograms that we augmented by using the script described before. After the augmentation, our training set contained 440 pictograms, 20 from each pictogram. For the validation set, we used 10 different pictograms that we augmented, as we did with the training set, making the total size of the validation set to 200. Due to the little amount of data, the best performance was reached at 10 iterations with 100\% accuracy on the validation set. After that point, the model started overfitting the training data and the accuracy of the validation set dropped to 60\% for 100 iterations. 

After we have reached the maximum accuracy, we took 100 different non-augmented pictograms and formed the training set. Using the weights obtained with the 10 iterations we once again obtained 100\% accuracy, given that the algorithm recognized correctly all 100 pictograms. For comparison, we ran the algorithm with the same test set and the weights obtained with 100 iterations. From the 100 pictograms, the algorithm correctly recognized only 17 of them, showing that using 100 iterations we overfed the data from the training set.

The problems we faced, which was the main reason why we used only 22 pictograms (440 images with the augmentation) and only 100 pictograms for the test set, were the following. First, we couldn't load all the pictograms we obtained from ARASAAC to try it at once because of a lack of system memory. The second problem was the time the model takes to make a prediction. To correctly compute how similar a given pictogram is to every other pictogram in the test set, the model requires pairs of images. A pair is constructed by the pictogram we want to recognize and one of the test sets. Mapping every pictogram of the test set to the given one creates the list of pairs we give to the algorithm to make predictions on.

The objective is to have all pictograms of the ARASAAC dataset to the test set so that we could compare a given pictogram against all others.




